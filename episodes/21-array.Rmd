---
title: Parallelising with Job Arrays.
teaching: 15
exercises: 5
---

```{r, echo=FALSE}
# Source the external configuration script
source("load_config.R")
```

::::::::::::::::::::::::::::::::::::::: objectives

- Prepare a job submission script for an array job.

::::::::::::::::::::::::::::::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::::: questions

- What are job arrays?
- What benefit does job arrays bring?
- What type of jobs would benefit from job arrays?

::::::::::::::::::::::::::::::::::::::::::::::::::


Parallel computing is a technique used to divide big tasks into smaller ones that can be solved simultaneously. Parallelism can be accomplished in different ways and it depends on the tasks that needs doing as well as the algorithms implemented to perform these tasks.

One way of implementing parallel computing is to distribute a job across multiple processors. This is usually accomplished by using the Message Passage Interface (MPI) which is a standardised way for CPU cores to communicate with one another while working together on a task. Software has to be written specifically to utilize MPI to take advantage of this.

Another form of parallel computing is an array job. This type of job is advantages if the same software has to be run across several files. An example of this would be in bioinformatics where the same workflow has to applied to a set of files that contain data for different samples. There is no need for the different jobs to "talk" to one another while they run. The advantage only lies in the fact that the jobs can run in parallel. One could potentially run such processes manually across different computers but imagine having a hundred files and each taking two hours to complete. You can run them in series which would take two hundred hours or you can manually start them across, say, four computers which would mean it would take 25 hours. But it would take you some time to start all these jobs if you do it manually. To complicate matters things will quite often go wrong and workflow won't complete in which case you have to first notice this, correct the problem and then restart it all.

Array jobs are controlled by the Slurm scheduler. You will need only one set of scripts to which you supply a list of files. Slurm will automatically distribute the jobs across available nodes. If any of the jobs fail you can easily restart the jobs to execute only on the files that failed.

cd training directory
mkdir username
cd username


Download the word frequency script
wget https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/word-freq.sh

write a small file to test our script

```bash
`r config$remote$prompt` nano test-data.txt
`r config$remote$prompt` cat test-data.txt
```

```bash
This is a small file - it will be very useful for trying out our script.
Some words are repeated in this file
- we can look for repeated words
and count them (to see which words are repeated most often).
```

Run at command line with a small test file (/nobackup/proj/comet_training/ArrayJob/test-data.txt)


Write a batch script to call the word-freq.sh as single job and run using slurm (job_single_word-freq.sh) 
or download it from https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/job_single_word-freq.sh

Download the data using https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/make-data.sh

Update the batch sript to run on the first file of 4 text files 
named data.1 data.2 data.3 data.4 
downloaded from the Guteberg project (in /nobackup/proj/comet_training/ArrayJob/ - see make-data.sh to download again)

Write a batch script to call the word-freq.sh as an array job with 4 parallel jobs to process all 4 text files (job_array_word-freq.sh)
or download it from https://github.com/NewcastleRSE-Training/hpc-intro/blob/main/episodes/files/job_array_word-freq.sh

Check slurm.out files to see how quickly the jobs completed

```bash
`r config$remote$bash_shebang`
`r config$sched$comment` `r config$sched$flag$queue``r config$sched$partition`
`r config$sched$comment` `r config$sched$flag$name``r config$sched$job_name1`
`r config$sched$comment` `r config$sched$flag$nodes`1
`r config$sched$comment` `r config$sched$flag$tasks`1
`r config$sched$comment` `r config$sched$flag$cpu`1

# Do a word frequency analysis of the collected works of Shakespeare

DATA_FILE=data.1

echo "Starting word frequency analysis of $DATA_FILE"
echo "=============================================="

time cat $DATA_FILE | \
	sed s'/\ /\n/g' | \
	tr -c -d "[A-Za-z\n]" | \
	tr [A-Z] [a-z] | \
	sort | \
	strings -n 1 | \
	uniq -c | \
	sort -n > data.out

echo "====================================="
echo "Completed word analysis of $DATA_FILE"
```

:::::::::::::::::::::::::::::::::::::::: keypoints

- Parallel computing allows applications to 

::::::::::::::::::::::::::::::::::::::::::::::::::
